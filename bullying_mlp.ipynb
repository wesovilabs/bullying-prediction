{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File path for the train data files and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = './data/train/range_14_16_men/'\n",
    "TEST_DATA_PATH = './data/test/range_14_16_men_test_data.csv'\n",
    "\n",
    "#Parameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 800\n",
    "display_step = 100\n",
    "\n",
    "# Number of records per file * number of train data files\n",
    "train_data_records = 1428368\n",
    "training_epochs=1\n",
    "\n",
    "# Network Parameters\n",
    "# Number of input nodes\n",
    "num_features = 10\n",
    "# Number of outpues \n",
    "n_classes = 1\n",
    "#Number of nodes in the first hidden layer\n",
    "n_hidden_1 =  8\n",
    "#Number of nodes in the second hidden layer\n",
    "n_hidden_2 =  4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will read the train data from different files and it will format the columns. In this case all the input data will be int numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1,name='train_data')\n",
    "    key, value = reader.read(filename_queue)\n",
    "\n",
    "\n",
    "    record_defaults = [\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32),\n",
    "        tf.constant([], dtype=tf.float32)\n",
    "    ]\n",
    "\n",
    "    Xi1, Xi2, Xi3, Xi4, Xi5, Xi6, Xi7, Xi8, Xi9, Xi10, Xo = \\\n",
    "        tf.decode_csv(value, record_defaults=record_defaults, field_delim=',')\n",
    "\n",
    "    features = tf.pack([ Xi1, Xi2, Xi3, Xi4, Xi5, Xi6, Xi7, Xi8, Xi9, Xi10])\n",
    "    label = tf.pack([ Xo])\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know we have multiples input files, and we will reading set of 800 records each time (this is defined in the parameters section => batch_size = 800).The way to do this is the below (recommended by Tensorflow) For more info have a look at [Tensorflow documentation](https://www.tensorflow.org/how_tos/reading_data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size, read_threads, num_epochs=None):\n",
    "  filename_queue = tf.train.string_input_producer(\n",
    "      filenames, num_epochs=num_epochs, shuffle=True)\n",
    "  example_list = [read_my_file_format(filename_queue) for _ in range(read_threads)]\n",
    "  min_after_dequeue = 10000\n",
    "  capacity = min_after_dequeue + 3 * batch_size\n",
    "  example_batch, label_batch = \\\n",
    "      tf.train.shuffle_batch_join(example_list, batch_size=batch_size, capacity=capacity,min_after_dequeue=min_after_dequeue)\n",
    "  return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below method we obtaine the *activation function*. Since we target will be a single neuron whose value is between 0 and 1, the activation function that will be using is a sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['h1']), biases['b1']), name=\"layer_1\")\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']), name=\"layer_2\")\n",
    "    out_layer = tf.matmul(layer_2, weights['out'])+ biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    x = tf.placeholder(tf.float32, [None, num_features])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([num_features, n_hidden_1]), dtype=tf.float32),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), dtype=tf.float32),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), dtype=tf.float32)\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.ones([n_hidden_1]), dtype=tf.float32),\n",
    "        'b2': tf.Variable(tf.ones([n_hidden_2]), dtype=tf.float32),\n",
    "        'out': tf.Variable(tf.ones([n_classes]), dtype=tf.float32)\n",
    "    }\n",
    "\n",
    "\n",
    "    # Construct model\n",
    "    pred = multilayer_perceptron(x,weights, biases)\n",
    "\n",
    "\n",
    "    # Define cost and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(pred, y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_prediction = tf.equal(tf.round(tf.nn.sigmoid(pred)), tf.round(y))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    train_data_files = [os.path.join(TRAIN_DATA_PATH, i) for i in os.listdir(TRAIN_DATA_PATH)]\n",
    "    batch_x, batch_y = input_pipeline(train_data_files, batch_size, 10)\n",
    "\n",
    "    # Timing\n",
    "    startTime = datetime.datetime.now()\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Initialize the variables (like the epoch counter).\n",
    "        sess.run(init)\n",
    "\n",
    "        # Start input enqueue threads.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0\n",
    "            total_batch = int(train_data_records / batch_size)\n",
    "            print (\"Total batch units \", total_batch)\n",
    "            for i in range(total_batch):\n",
    "                b_x, b_y = sess.run([batch_x, batch_y])\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: b_x, y: b_y})\n",
    "                avg_cost += c / total_batch\n",
    "                if i % display_step == 0:\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: b_x, y: b_y})\n",
    "                    print(\"Epoch: %03d/%03d, iteration %03d/%03d,   cost: %.9f\" % (epoch+1, training_epochs,i,total_batch,avg_cost))\n",
    "                    train_acc = sess.run(accuracy, feed_dict={x: b_x, y: b_y})\n",
    "                    print(\" Training accuracy: %.3f\" % (train_acc))\n",
    "\n",
    "            print(\"Epoch: %03d/%03d cost: %.9f\" % (epoch+1, training_epochs, avg_cost))\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        endTime = datetime.datetime.now()\n",
    "        fitTime = (endTime - startTime)\n",
    "        print(\"Training Time:\", fitTime)\n",
    "\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        # Test model\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: b_x, y: b_y})\n",
    "        print(\"Epoch: %03d/%03d, iteration %03d/%03d,   cost: %.9f\" % (epoch, training_epochs, i, total_batch, avg_cost))\n",
    "        train_acc = sess.run(accuracy, feed_dict={x: b_x, y: b_y})\n",
    "        print(\" Training accuracy: %.3f\" % (train_acc))\n",
    "        test_data = np.genfromtxt(TEST_DATA_PATH, delimiter=',')\n",
    "        print(\"final accuracy is \", sess.run(accuracy, feed_dict={x: test_data[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], y: test_data[:, [10]]})*100,\"%\")\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print predictions test resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_result(test_data, prediction):\n",
    "    success_predictions = 0\n",
    "    for row in range(0, 10):\n",
    "        print(\"Person \", row+1, ' of ', 10)\n",
    "        print(\" Is short: \", test_data.item(row,0))\n",
    "        print(\" Shy: \", test_data.item(row, 1))\n",
    "        print(\" Wear braces: \", test_data.item(row, 2))\n",
    "        print(\" Wear glasses: \", test_data.item(row, 3))\n",
    "        print(\" Is less popular: \", test_data.item(row, 4))\n",
    "        print(\" Different race: \", test_data.item(row, 5))\n",
    "        print(\" Low socioeconomic status: \", test_data.item(row, 6))\n",
    "        print(\" Gay or lesbian: \", test_data.item(row, 7))\n",
    "        print(\" Have disability: \", test_data.item(row, 8))\n",
    "        print(\" Overweight: \", test_data.item(row, 9))\n",
    "        print(\" Suffer bullying: \", test_data.item(row, 10))\n",
    "        print(\"-----------------\")\n",
    "        print(\" Prediction: \", prediction.item(row))\n",
    "        print(\"-----------------\")\n",
    "        if ((test_data.item(row, 10)) == (int)(prediction.item(row))):\n",
    "            success_predictions+=1\n",
    "    print(\"\\n\\n Success predictions \", success_predictions,\" of \",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batch units  1785\n",
      "Epoch: 001/001, iteration 000/1785,   cost: 0.000321467\n",
      " Training accuracy: 0.752\n",
      "Epoch: 001/001, iteration 100/1785,   cost: 0.031678581\n",
      " Training accuracy: 0.752\n",
      "Epoch: 001/001, iteration 200/1785,   cost: 0.062657940\n",
      " Training accuracy: 0.760\n",
      "Epoch: 001/001, iteration 300/1785,   cost: 0.093069139\n",
      " Training accuracy: 0.731\n",
      "Epoch: 001/001, iteration 400/1785,   cost: 0.122508746\n",
      " Training accuracy: 0.761\n",
      "Epoch: 001/001, iteration 500/1785,   cost: 0.150295287\n",
      " Training accuracy: 0.745\n",
      "Epoch: 001/001, iteration 600/1785,   cost: 0.175484577\n",
      " Training accuracy: 0.796\n",
      "Epoch: 001/001, iteration 700/1785,   cost: 0.197306821\n",
      " Training accuracy: 0.874\n",
      "Epoch: 001/001, iteration 800/1785,   cost: 0.215796835\n",
      " Training accuracy: 0.923\n",
      "Epoch: 001/001, iteration 900/1785,   cost: 0.231472315\n",
      " Training accuracy: 0.946\n",
      "Epoch: 001/001, iteration 1000/1785,   cost: 0.244853402\n",
      " Training accuracy: 0.955\n",
      "Epoch: 001/001, iteration 1100/1785,   cost: 0.256367770\n",
      " Training accuracy: 0.952\n",
      "Epoch: 001/001, iteration 1200/1785,   cost: 0.266229734\n",
      " Training accuracy: 0.957\n",
      "Epoch: 001/001, iteration 1300/1785,   cost: 0.274886144\n",
      " Training accuracy: 0.979\n",
      "Epoch: 001/001, iteration 1400/1785,   cost: 0.282407029\n",
      " Training accuracy: 0.979\n",
      "Epoch: 001/001, iteration 1500/1785,   cost: 0.289152982\n",
      " Training accuracy: 0.983\n",
      "Epoch: 001/001, iteration 1600/1785,   cost: 0.295122057\n",
      " Training accuracy: 0.981\n",
      "Epoch: 001/001, iteration 1700/1785,   cost: 0.300451048\n",
      " Training accuracy: 1.000\n",
      "Epoch: 001/001 cost: 0.304471673\n",
      "Training complete!\n",
      "Training Time: 0:04:12.229073\n",
      "Epoch: 000/001, iteration 1784/1785,   cost: 0.304471673\n",
      " Training accuracy: 0.991\n",
      "final accuracy is  100.0 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
